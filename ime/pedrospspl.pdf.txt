Universidade de SÃ£o Paulo
Instituto de MatemÃ¡tica e EstatÃ­stica
Bacharelado em CiÃªncia da ComputaÃ§Ã£o

Treinamento e SÃ­ntese de FCNs
com Campo Receptivo VariÃ¡vel
Um estudo aplicado a
segmentaÃ§Ã£o de imagens
Pedro Sola Pimentel

Monografia Final
mac 499 â€” Trabalho de
Formatura Supervisionado

Supervisor: Prof. Dr. Roberto Hirata

SÃ£o Paulo
5 de Dezembro de 2019

Resumo
Pedro Sola Pimentel. Treinamento e SÃ­ntese de FCNs com Campo Receptivo VariÃ¡vel:
Um estudo aplicado a segmentaÃ§Ã£o de imagens. Monografia (Bacharelado). Instituto de
MatemÃ¡tica e EstatÃ­stica, Universidade de SÃ£o Paulo, SÃ£o Paulo, 2019.

Neste trabalho exploramos o uso de Redes Neurais Totalmente Convolucionais para segmentaÃ§Ã£o de imagens combinando diferentes campos receptivos de uma forma unificada. Criamos
uma arquitetura de redes neurais que combina estes campos receptivos em um Ãºnico modelo
de segmentaÃ§Ã£o. Nos testes realizados, este modelo obteve performance superior aos modelos
individuais de segmentaÃ§Ã£o. AlÃ©m disso, conseguimos obter resultados melhores que o modelo
com Ã¡rea de campo receptivo equivalente em uma parte dos exemplos considerados. Afim de
prover uma leitura concisa e suficiente, apresentamos todos os conceitos bÃ¡sicos utilizados
durante o projeto, assim como detalhes de assuntos que cerceiam o trabalho e podem ser de
interesse do leitor.
Palavras-chave: SegmentaÃ§Ã£o. Aprendizado-de-Maquina. Deep-Learning. Redes-Neurais.
FCNs. ConvoluÃ§Ãµes.

Abstract
Pedro Sola Pimentel. Training and Synthesis of FCNs with Variable Receptive Field:
A study applied to image segmentation. Capstone Project Report (Bachelor). Institute of
Mathematics and Statistics, University of SÃ£o Paulo, SÃ£o Paulo, 2019.

Throughout this work we explore the use of Fully Convolutional Networks for image segmentation combining different receptive fields in a unified manner. Weâ€™ve created a Neural
Network architecture that combines these receptive fields in one single segmentation model.
On the tests, this model achieved superior performance than the individual segmentation
models. Also, it was able to achieve better results than the model with equivalent receptive
field on part of the considered examples. Aiming to provide a concise and self-sufficient read,
we present all the basic concepts used on the project, as well as details on topics that talk to
this project and may be of interest to the reader.
Keywords: Segmentation. Machine-Learning. Deep-Learning. Neural-Networks. FCNs.
Convolution.

v

Lista de Figuras
1.1
1.2

SegmentaÃ§Ã£o de uma imagem de aviÃ£o. . . . . . . . . . . . . . . . . . . .
Ideia de campo receptivo variÃ¡vel: Ã€ esquerda 5 campos receptivos que
juntos totalizam a Ã¡rea captada pelo campo receptivo da direita. . . . . .

1

3.1

Um exemplo de convoluÃ§Ã£o. . . . . . . . . . . . . . . . . . . . . . . . . .

14

4.1
4.2
4.3

Esquema final desejado. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Rede convolucional de segmentaÃ§Ã£o. . . . . . . . . . . . . . . . . . . . . .
Rede neural de SÃ­ntese. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17
18
20

1

Resultado final do trabalho: Entrada Ã  esquerda, Ground Truth ao centro e
saÃ­da da combinaÃ§Ã£o de redes Ã  direita. . . . . . . . . . . . . . . . . . . .
Entrada e saÃ­da dos modelos: Ã€ esquerda temos o quadrinho e a direita o
Ground Truth. A segunda imagem representa a saÃ­da esquema de Redes
Neurais e a terceira imagem a saÃ­da obtida atravÃ©s do mÃ©todo de voto
ponderado . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Resultados das redes individuais aplicados a um exemplo de imagem. . .
DistribuiÃ§Ã£o de acurÃ¡cias de cada um dos modelos. Da esquerda para a
direita temos: Rede Neural, Voto Ponderado e os classificadores individuais
na ordem descrita na seÃ§Ã£o 5. . . . . . . . . . . . . . . . . . . . . . . . . .

2

3
4

2

25

25
26

26

vii

SumÃ¡rio
1 IntroduÃ§Ã£o

1

2 Conceitos Preliminares
2.1 SegmentaÃ§Ã£o de Imagens . . . . . . . . .
2.2 Conceitos de Aprendizado de MÃ¡quina .
2.2.1 Aprendizado Supervisionado . .
2.2.2 MÃ©tricas de Performance ou Erro
2.2.3 AvaliaÃ§Ã£o e OtimizaÃ§Ã£o Modelos

.
.
.
.
.

5
5
6
6
7
9

.
.
.
.
.
.
.
.

11
11
12
13
13
14
14
15
15

.
.
.
.
.
.

17
17
18
18
19
19
20

.
.
.
.
.

3 Redes Neurais
3.1 FunÃ§Ãµes Feed-Forward . . . . . . . . . . .
3.2 Backpropagation . . . . . . . . . . . . . . .
3.3 FunÃ§Ãµes de AtivaÃ§Ã£o . . . . . . . . . . . . .
3.3.1 Softmax . . . . . . . . . . . . . . .
3.3.2 Rectified Linear Unit (ReLU) . . . .
3.4 Redes Neurais Convolucionais . . . . . . .
3.5 OperaÃ§Ãµes de ConvoluÃ§Ã£o . . . . . . . . .
3.6 Redes Neurais Totalmente Convolucionais
4 Metodologia
4.1 SegmentaÃ§Ã£o . . . . . . . . . . . . .
4.1.1 Arquitetura de SegmentaÃ§Ã£o
4.2 SÃ­ntese . . . . . . . . . . . . . . . .
4.2.1 Voto MajoritÃ¡rio . . . . . .
4.2.2 Voto Ponderado . . . . . . .
4.2.3 Rede Neural de SÃ­ntese . . .

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

5 Resultados

21

6 ConclusÃ£o

23

viii

ApÃªndices
Anexos

ReferÃªncias

27

1

CapÃ­tulo 1
IntroduÃ§Ã£o
A recente disponibilizaÃ§Ã£o de recursos computacionais atravÃ©s de plataformas online, denominadas nuvem, juntamente com a diminuiÃ§Ã£o de preÃ§os e aumento de poder
de processamento possibilitou grandes avanÃ§os em tÃ³picos relacionados a InteligÃªncia
Artificial. Tarefas que antes seriam feitas por profissionais com conhecimento especÃ­fico
da Ã¡rea hoje sÃ£o realizadas por algoritmos de aprendizado, o que possibilita que estes
mesmos profissionais foquem seus esforÃ§os em outras Ã¡reas do conhecimento e dediquem
seu tempo Ã s tarefas mais importantes da profissÃ£o.
Em sua maioria, algoritmos de aprendizado dependem de uma gama de exemplos para
performar a tarefa desejada. Neste sentido, parte da automatizaÃ§Ã£o das tarefas relacionadas Ã  imagens se deve Ã  crescente disponibilizaÃ§Ã£o de conjunto de dados deste tipo.
Cada imagem funciona como uma fonte de informaÃ§Ã£o para o algoritmo, que emprega
transformaÃ§Ãµes como remoÃ§Ã£o de ruÃ­do e destaque de contorno a fim de realÃ§ar objetos de
interesse. Geralmente essas transformaÃ§Ãµes atuam localmente, em regiÃµes centradas em
um pixel e delimitadas por uma janela.
Dentre estas tarefas, podemos descrever desde as mais comuns Ã s mais complexas,
variando de segmentaÃ§Ã£o de dÃ­gitos escritos Ã  mÃ£o (Lu e Shridhar, 1996) atÃ© detecÃ§Ã£o de
tumores em exames (Mustaqeem et al., 2012). Para isso, os mais diversos algoritmos sÃ£o
empregados, tais como: regressÃ£o linear, clustering, redes neurais, redes neurais convolucionais e SVM. Aqui vamos explorar o problema de segmentaÃ§Ã£o de imagens, que de forma
resumida significa classificar pixels de uma imagem, de forma a identificar entidades que
possuem alguma caracterÃ­stica em comum.

Figura 1.1: SegmentaÃ§Ã£o de uma imagem de aviÃ£o.

2
1 | INTRODUÃ‡ÃƒO

Para isso utilizamos redes convolucionais para treinar diferentes classificadores que
utilizam janelas com centro ligeiramente deslocado. Esta ideia veio da seguinte observaÃ§Ã£o:
Por um lado Ã© matematicamente claro que quanto maior o campo de captura de um classificador de pixels, melhor sua capacidade de discriminaÃ§Ã£o, visto que temos mais informaÃ§Ã£o
para realizar a prediÃ§Ã£o. Na prÃ¡tica, contudo, o tamanho de um campo receptivo Ã© limitado
em funÃ§Ã£o da quantidade de dados de treinamento (quanto maior o campo receptivo,
menor a quantidade de amostras) e a complexidade de rede, visto que a complexidade do
classificador cresce conforme aumentamos o campo receptivo, precisando assim de mais
exemplos para se obter um classificador robusto. Este fato acaba criando um trade-off entre
erro de generalizaÃ§Ã£o e erro de completude de amostra.

Figura 1.2: Ideia de campo receptivo variÃ¡vel: Ã€ esquerda 5 campos receptivos que juntos totalizam
a Ã¡rea captada pelo campo receptivo da direita.

Quando combinamos vÃ¡rios classificadores que sÃ£o baseados em janelas com os centros
ligeiramente deslocadas em relaÃ§Ã£o ao pixel-alvo, podemos pensar que a combinaÃ§Ã£o estÃ¡
tendo acesso a um conjunto maior de pixels em torno do pixel-alvo, embora cada um dos
classificadores tenha acesso a apenas um subconjunto deles. A Figura 1.1 ilustra esse fato.
Em seu artigo Hirata, 2008 utiliza classificadores morfolÃ³gicos para realizar este mesmo
procedimento. Aqui queremos estudar o uso de redes convolucionais para realizar a tarefa
de segmentaÃ§Ã£o. Com base nos experimentos jÃ¡ realizados na literatura esperamos que a
combinaÃ§Ã£o destes classificadores resulte em um classificador que tenha desempenho ao
menos igual ao melhor desempenho dentre os classificadores individuais que fazem parte
da combinaÃ§Ã£o.
Neste trabalho aplicamos segmentaÃ§Ã£o de imagens em um conjunto de imagens de
mangÃ¡s (revistas no estilo de quadrinhos japonÃªs). O objetivo Ã© extrair os textos de cada
uma das pÃ¡ginas disponÃ­veis de forma a reduzir o erro total em relaÃ§Ã£o a imagem ideal. Mais
detalhes sobre o conjunto de imagens, tamanho de amostra e detalhes de implementaÃ§Ã£o
serÃ£o discutidos no capÃ­tulo 5.
Ao decorrer do texto daremos detalhes sobre os conceitos empregados neste processo,
arquitetura utilizada e background matemÃ¡tico mÃ­nimo para entendimento do trabalho.
Caso o leitor tenha interesse em algum dos assuntos que cerceiam o conteÃºdo disponibilizado aqui, Ã© recomendado a leitura do conteÃºdo citado na seÃ§Ã£o de bibliografia, assim
como revistas acadÃªmicas relacionadas.

3
1 | INTRODUÃ‡ÃƒO

Este texto estÃ¡ organizado da seguinte maneira: O capÃ­tulo 2 dÃ¡ uma visÃ£o mais ampla
sobre a tarefa que iremos explorar e os conceitos bÃ¡sicos de aprendizado de mÃ¡quina,
enquanto que o CapÃ­tulo 3 introduz os conceitos tÃ©cnicos e matemÃ¡ticos especÃ­ficos aos
algoritmos que utilizamos no trabalho. Em seguida tocamos na metodologia do trabalho
em si, como detalhes de arquitetura e fluxo de informaÃ§Ãµes. Feito isso, apresentamos os
resultados obtidos atravÃ©s da tÃ©cnica escolhida no conjunto de dados especificado.

5

CapÃ­tulo 2
Conceitos Preliminares
2.1

SegmentaÃ§Ã£o de Imagens

Imagens digitais sÃ£o uma coleÃ§Ã£o de pixels. Estes elementos sÃ£o em geral arranjados em
uma grade discreta retangular ğº âŠ† ğ‘ 2 e possuem, cada um, uma coordenada de linha ğ‘–, uma
de coluna ğ‘— e valores de intensidade de cor associados. Desta forma, podemos descrever
uma imagem matematicamente como uma funÃ§Ã£o ğ‘“ (ğº):
ğ‘“ âˆ¶ ğ‘2 â†’ ğ¾ğ‘›
onde ğ¾ Ã© o conjunto das intensidades e ğ‘› o nÃºmero de bandas. Tipicamente uma imagem
Ã© dividida nas bandas vermelho, verde e azul sendo assim denominadas RGB. JÃ¡ as
intensidades de cor geralmente variam no intervalo [0, 255].
AlÃ©m de intensidades, pixels possuem uma relaÃ§Ã£o de vizinhanÃ§a com outros pixels.
Comumente consideram-se as vizinhanÃ§as 4 ou 8. Os 4-vizinhos baseiam-se na conectividade 4, que estabelece que a conectividade Ã© definida por adjacÃªncia vertical ou horizontal.
JÃ¡ os 8-vizinhos sÃ£o definidos pela conectividade 8, estabelecida como a adjacÃªncia nÃ£o
sÃ³ na horizontal e vertical, mas tambÃ©m nas diagonais. O conjunto de pixels vizinhos de
um pixel ğ‘ serÃ¡ denotado ğ‘‰ (ğ‘), e depende da conectividade considerada. Dizemos que um
pixel ğ‘ Ã© conectado a outro ğ‘ âŸº ğ‘ âˆˆ ğ‘‰ (ğ‘). Um conjunto de pixels conectados, isto Ã©, tal
que existe um caminho conectando quaisquer dois pixels do conjunto, Ã© uma regiÃ£o ou
componente de uma imagem.
O termo segmentaÃ§Ã£o Ã© descrito como o ato de segmentar, ou fracionar algum conjunto ou populaÃ§Ã£o em partes que compartilham uma mesma caracterÃ­stica. Ãreas como
SegmentaÃ§Ã£o de Mercado e SegmentaÃ§Ã£o de Marketing sÃ£o alguns exemplos onde a tÃ©cnica
de segmentaÃ§Ã£o pode ser empregada, nestes casos o interesse em questÃ£o Ã© geralmente
criar planos ou campanhas especÃ­ficas para cada um dos segmentos criados, trazendo um
desempenho melhor devido Ã  especificidade.
A segmentaÃ§Ã£o de imagens consiste em particionar um conjunto de pixels em regiÃµes
nÃ£o vazias, duas a duas disjuntas, de tal forma que a uniÃ£o destas regiÃµes cubra todos os
pixels da imagem. Em geral a granularidade, ou especificidade, da segmentaÃ§Ã£o Ã© estabele-

6
2 | CONCEITOS PRELIMINARES

cida baseada na aplicaÃ§Ã£o em questÃ£o. Cada uma dessas regiÃµes apresenta uma mesma
caracterÃ­stica, de interesse do pesquisador, como cor, textura ou objeto descrito. Em algumas
aplicaÃ§Ãµes o interesse consiste em obter regiÃµes maximais com cores homogÃªneas, sem
necessariamente uma interpretaÃ§Ã£o semÃ¢ntica associada Ã s regiÃµes. Em outras situaÃ§Ãµes,
espera-se que as regiÃµes correspondam a objetos ou partes de objetos que constituem o
alvo de um estudo. Por exemplo, se o estudo refere-se Ã  anÃ¡lise de imagens de cÃ©lulas,
pode ser do interesse simplesmente separar cÃ©lulas do restante, mas tambÃ©m pode ser do
interesse segmentar cÃ©lulas e seus nÃºcleos simultaneamente.

2.2

Conceitos de Aprendizado de MÃ¡quina

Existem diferentes algoritmos para segmentaÃ§Ã£o de imagens. No entanto, em essÃªncia,
o propÃ³sito de todos eles Ã© o mesmo: o de agrupar ou classificar pixels de forma que o
rÃ³tulo de classificaÃ§Ã£o dos pixels seja suficiente para identificar as regiÃµes da segmentaÃ§Ã£o.
Essa tarefa de classificaÃ§Ã£o de pixels pode ser realizada por algoritmos de aprendizado
de mÃ¡quina. Em uma abordagem mais tradicional, tipicamente extraem-se caracterÃ­sticas
diversas da imagem, em geral em torno de cada pixel, e essas caracterÃ­sticas sÃ£o utilizadas
como uma representaÃ§Ã£o do pixel para efeitos de classificaÃ§Ã£o. As caracterÃ­sticas podem ser
informaÃ§Ãµes relacionadas Ã s intensidades de cor, textura, localizaÃ§Ã£o ou outra caracterÃ­stica
relativa ao pixel ou conjunto de pixels.
O termo Aprendizado de MÃ¡quina foi introduzido inicialmente por Solomonoff, 1957.
No artigo An Inductive Inference Machine o autor descreve Aprendizado de MÃ¡quina da
seguinte maneira: "Uma mÃ¡quina desenhada para aprender a resolver problemas matemÃ¡ticos
atravÃ©s de uma sÃ©rie de exemplos resolvidos corretamente.". Contudo devido aos avanÃ§os
recentes na Ã¡rea podemos redefinir Aprendizado de mÃ¡quina, sem perda de generalidade,
como uma aplicaÃ§Ã£o de InteligÃªncia Artificial (IA) que provÃª a sistemas a habilidade de
aprender e melhorar atravÃ©s da experiÃªncia, sem ser explicitamente programados.
Ainda que complicado Ã  primeira vista, este pode ser abstraÃ­do como uma tarefa simples
de encontrar padrÃµes nos dados. Assim como o raciocÃ­nio bÃ¡sico, estes algoritmos levam
em consideraÃ§Ã£o as particularidades de objetos, conceitos e definiÃ§Ãµes e podem ou nÃ£o
assumir distribuiÃ§Ãµes nos dados. No caso de uma regressÃ£o linear por exemplo, assumimos
que a distribuiÃ§Ã£o dos dados Ã© linear, e a partir daÃ­ criamos maneiras rÃ¡pidas de calcular a
reta que melhor descreve os dados segundo alguma mÃ©trica definida.
Tradicionalmente dividimos os algoritmos de aprendizado como aprendizado supervisionado, aprendizado nÃ£o-supervisionados, e aprendizado por reforÃ§o.

2.2.1

Aprendizado Supervisionado

Denominamos como problemas de aprendizado supervisionado aplicaÃ§Ãµes em que
o conjunto de treino consiste de um conjunto de vetores de entrada e um conjunto de
vetores de saÃ­da correspondente. Problemas onde o vetor de saÃ­da consiste de uma ou
mais variÃ¡veis contÃ­nuas sÃ£o denominados problemas de regressÃ£o. Um exemplo desta
categoria Ã© a previsÃ£o de demanda de um produto de acordo com uma sÃ©rie de variÃ¡veis
temporais, como mÃªs, temperatura e presenÃ§a de campanhas de marketing.

7
2.2 | CONCEITOS DE APRENDIZADO DE MÃQUINA

No caso de segmentaÃ§Ã£o de imagens, a forma mais simples de prover esses vetores Ã©
criar pares de imagem consistindo de uma imagem de entrada (um exemplar do tipo de
imagens que se deseja segmentar) e a correspondente imagem esperada apÃ³s a segmentaÃ§Ã£o.
Essa imagem esperada Ã© tipicamente preparada editando-se manualmente uma imagem de
entrada.
De posse desses pares, pode-se entÃ£o extrair os dados de treinamento. Para cada pixel,
a partir da imagem de entrada constrÃ³i-se o vetor x de caracterÃ­sticas e, este, juntamente
com o valor y do mesmo pixel na imagem esperada, forma um exemplo de treinamento.
Juntos, esses exemplos formam um conjunto de treinamento.
Consideramos o caso de segmentaÃ§Ã£o como um problema de classificaÃ§Ã£o pixel a pixel.
Isto Ã©, para cada pixel no mapa de uma imagem definimos uma categoria correspondente.
Neste sentido, podemos dizer que a segmentaÃ§Ã£o Ã© um exemplo de um problema de
classificaÃ§Ã£o. Nesta categoria se encaixam os seguintes algoritmos:
â€¢ Redes Neurais
â€¢ MÃ¡quinas de Vetor de Suporte (SVMs)
â€¢ Classificadores Naive Bayes
â€¢ Ãrvores de decisÃ£o

2.2.2

MÃ©tricas de Performance ou Erro

Para o treinamento de um algoritmo de aprendizado se faz necessÃ¡rio o uso de mÃ©tricas
de distÃ¢ncia. AtravÃ©s do processo de otimizaÃ§Ã£o desta mÃ©trica Ã© que um modelo aprende os
padrÃµes em um conjunto de dados. Definimos como mÃ©trica uma funÃ§Ã£o ğ‘‘ âˆ¶ ğ‘‹ Ã—ğ‘‹ â†’ [0, âˆ),
comumente nominada como distÃ¢ncia, que satisfaz as seguintes propriedades para todo
(ğ‘¥, ğ‘¦, ğ‘§) âˆˆ ğ·:
â€¢ ğ‘‘ nÃ£o negativo:

ğ‘‘(ğ‘¥, ğ‘¦) â‰¥ 0âˆ€

â€¢ identidade:

ğ‘‘(ğ‘¥, ğ‘¦) = 0 âŸº ğ‘¥ = ğ‘¦

â€¢ ğ‘‘ simÃ©trico:

ğ‘‘(ğ‘¥, ğ‘¦) = ğ‘‘(ğ‘¦, ğ‘¥)

â€¢ desigualdade triangular:

ğ‘‘(ğ‘¥, ğ‘¦) â‰¤ ğ‘‘(ğ‘¥, ğ‘§) + ğ‘‘(ğ‘§, ğ‘¦)

A escolha de uma mÃ©trica apropriada de avaliaÃ§Ã£o de modelos Ã© um dos tÃ³picos de
grande interesse na Ã¡rea de aprendizado de mÃ¡quina. A decisÃ£o deve levar em consideraÃ§Ã£o
o tipo da tarefa, o conjunto de dados em qual etapa serÃ¡ utilizada a mÃ©trica.
Na literatura existem diferentes mÃ©tricas possÃ­veis para a avaliaÃ§Ã£o de um modelo.
Dentre elas, Ã© importante mencionar as seguintes:
AcurÃ¡cia
A acurÃ¡cia Ã© definida como a razÃ£o entre o nÃºmero de prediÃ§Ãµes e o nÃºmero total de
amostras, ou seja:
#ğ‘‘ğ‘’ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘œğ‘’ğ‘  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘¡ğ‘ğ‘ 
ğ´ğ‘ğ‘ =
#ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘‘ğ‘’ ğ‘’ğ‘¥ğ‘’ğ‘šğ‘ğ‘™ğ‘œğ‘ 

8
2 | CONCEITOS PRELIMINARES

Esta mÃ©trica de classificaÃ§Ã£o pode ser utilizada quando o conjunto de dados Ã© balanceado, caso contrÃ¡rio esta mÃ©trica pode levar a um viÃ©s no modelo, uma vez que nÃ£o temos
controle sobre o tamanho da amostra para cada uma das classes possÃ­veis.
Log loss
A mÃ©trica log loss busca penalizar as classificaÃ§Ãµes falsas, atravÃ©s do cÃ¡lculo utilizando
a probabilidade para cada uma das classes. Suponha que existem ğ‘ amostras e ğ‘€ classes,
entÃ£o vale :
âˆ’1 ğ‘ ğ‘€
ğ¿ğ‘œğ‘”ğ¿ğ‘œğ‘ ğ‘  =
âˆ‘ âˆ‘ ğ‘¦ğ‘–ğ‘— âˆ— ğ‘™ğ‘œğ‘”(ğ‘ğ‘–ğ‘— )
ğ‘ ğ‘–=1 ğ‘—=1
Onde:
1. ğ‘¦ğ‘–ğ‘— indica se a amostra ğ‘– pertence classe ğ‘— ou nÃ£o.
2. ğ‘ğ‘–ğ‘— indica a probabilidade da amostra pertencer Ã  classe ğ‘—
Esta mÃ©trica Ã© comumente utilizada para treinamento e avaliaÃ§Ã£o de classificadores multiclasse.
Entropia Cruzada
A entropia cruzada nada mais Ã© do que a aplicaÃ§Ã£o da mÃ©trica Log Loss para o caso
binÃ¡rio. Neste caso, a equaÃ§Ã£o se reduz ao seguinte cÃ¡lculo:
âˆ’1 ğ‘
ğ»ğ‘ =
âˆ‘ ğ‘¦ğ‘– âˆ— ğ‘™ğ‘œğ‘”(ğ‘(ğ‘¦ğ‘– )) + (1 âˆ’ ğ‘¦ğ‘– ) âˆ— ğ‘™ğ‘œğ‘”(1 âˆ’ ğ‘(ğ‘¦ğ‘– ))
ğ‘ ğ‘–=1
Esta mÃ©trica especÃ­fica Ã© utilizada para treinamento de classificadores com saÃ­da binÃ¡ria.
PrecisÃ£o, Recall e RazÃ£o F1
Estas mÃ©tricas sÃ£o utilizadas como medidas de teste de classificadores binÃ¡rios, onde os
testes sÃ£o separados em quatro classes: Verdadeiros Positivos ou TP, onde o classificador
classifica corretamente um caso verdadeiro, Falso Negativos FN, onde o classificador classifica uma amostra verdadeira como falsa, Falsos Positivos FP, onde o classificador classifica
uma amostra falsa como verdadeira e Verdadeiros Negativos TN, onde o classificador
classifica uma amostra falsa corretamente.
Em termos matemÃ¡ticos definimos PrecisÃ£o, Recall e F1 da seguinte maneira:
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ
ğ‘‡ğ‘ƒ
ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ =
ğ‘‡ğ‘ƒ + ğ¹ğ‘
1
ğ¹1 = 2 âˆ—
1
1
+ ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘ğ‘œ
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘ğ‘œ =

9
2.2 | CONCEITOS DE APRENDIZADO DE MÃQUINA

Por consistir de uma mÃ©dia harmÃ´nica entre precisÃ£o e recall, a mÃ©trica F1 exibe uma
boa avaliaÃ§Ã£o sobre quÃ£o preciso Ã© o classificador assim como quÃ£o robusto.
PrecisÃµes altas e recall baixo configuram um modelo de alta acurÃ¡cia mas pouca
generalizaÃ§Ã£o. Neste sentido, Ã© desejado um modelo um modelo com alto ğ¹ 1
Erro MÃ©dio Absoluto (MAE)
O erro mÃ©dio absoluto captura a mÃ©dia da diferenÃ§a entre o conjunto retornado pelo
modelo e o conjunto desejado, em termos absolutos. Definimos ğ‘€ğ´ğ¸ como:
ğ‘€ğ´ğ¸ =

1 ğ‘
âˆ‘ |ğ‘¦ğ‘– âˆ’ ğ‘¦Ì‚ğ‘– |
ğ‘ ğ‘–=1

Apesar de retornar uma boa medida sobre o quÃ£o longÃ­nquo estÃ¡ a prediÃ§Ã£o do conjunto
verdade, esta mÃ©trica nÃ£o retorna informaÃ§Ãµes sobre a direÃ§Ã£o do erro, e Ã© difÃ­cil calcular
o gradiente dessa funÃ§Ã£o. Este fato dificulta seu uso em algoritmos com backpropagation.
Erro MÃ©dio QuadrÃ¡tico (MSE)
Similar ao MAE, o erro mÃ©dio quadrÃ¡tico captura a mÃ©dia da diferenÃ§a entre o conjunto
retornado pelo modelo e o conjunto desejado, com a diferenÃ§a que este toma o quadrado
do erro entre os dois. Definimos o ğ‘€ğ‘†ğ¸ como:
1 ğ‘
ğ‘€ğ‘†ğ¸ = âˆ‘(ğ‘¦ğ‘– âˆ’ ğ‘¦Ì‚ğ‘– )2
ğ‘ ğ‘–=1
Com relaÃ§Ã£o ao MSE, esta mÃ©trica possibilita o uso do gradiente para minimizar o erro
de sistema preditivo. AlÃ©m disso, uma vez que o erro Ã© elevado ao quadrado, esta mÃ©trica
tende a valorizar erros maiores e desvalorizar erros menores.

2.2.3

AvaliaÃ§Ã£o e OtimizaÃ§Ã£o Modelos

No fluxo padrÃ£o de um modelo de aprendizado de mÃ¡quina, dividimos a avaliaÃ§Ã£o de
modelos em 3 etapas:
1. Treinamento
2. ValidaÃ§Ã£o
3. Teste
Em cada uma das avaliaÃ§Ãµes, o erro calculado Ã© utilizado de uma forma especÃ­fica.
Treinamento
O erro de treinamento quantifica qual a distÃ¢ncia entre a saÃ­da do modelo ğ‘“ (xğ’•ğ’“ğ’‚ğ’Šğ’ ) e a
saÃ­da esperada yğ’•ğ’“ğ’‚ğ’Šğ’ no conjunto de treinamento xğ’•ğ’“ğ’‚ğ’Šğ’ em relaÃ§Ã£o ao conjunto esperado

10
2 | CONCEITOS PRELIMINARES

ğ‘¦ atravÃ©s de uma mÃ©trica de erro escolhida ğ¸ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› . Este valor pode ou nÃ£o ser utilizado para
a otimizaÃ§Ã£o dos parÃ¢metros do modelo, a depender se o problema em questÃ£o possui ou
nÃ£o uma distribuiÃ§Ã£o conhecida. Em casos como regressÃ£o linear, a soluÃ§Ã£o Ã³tima pode
ser calculada sem a necessidade do cÃ¡lculo do erro, uma vez que a mÃ©trica SSR Ã© convexa
quando aplicada ao problema de regressÃ£o.
No capÃ­tulo 3.2 vamos mostrar como este erro Ã© utilizado para otimizar os parÃ¢metros
de uma rede neural, utilizando a tÃ©cnica de propagaÃ§Ã£o para trÃ¡s.
ValidaÃ§Ã£o
O erro de validaÃ§Ã£o quantifica qual a distÃ¢ncia entre a saÃ­da do modelo ğ‘“ (xğ’—ğ’‚ğ’ ) e a saÃ­da
esperada yğ’—ğ’‚ğ’ no conjunto de validaÃ§Ã£o xğ’—ğ’‚ğ’ em relaÃ§Ã£o ao conjunto esperado yğ’—ğ’‚ğ’ atravÃ©s
de uma mÃ©trica de erro escolhida ğ¸ğ‘£ğ‘ğ‘™ . Este valor Ã© monitorado a fim de prevenir que o
modelo tenha problemas de sobreajuste, ou seja, Ã© com este valor que podemos verificar se
em algum ponto perde-se a generalidade, de forma que o modelo fica muito especÃ­fico ao
conjunto de treino.
Teste
Similar aos demais, este erro quantifica qual a distÃ¢ncia entre a saÃ­da do modelo ğ‘“ (xğ’•ğ’†ğ’”ğ’• )
e a saÃ­da esperada yğ’•ğ’†ğ’”ğ’• no conjunto de teste xğ’•ğ’†ğ’”ğ’• em relaÃ§Ã£o ao conjunto esperado yğ’•ğ’†ğ’”ğ’•
atravÃ©s de uma mÃ©trica de erro escolhida ğ¸ğ‘¡ğ‘’ğ‘ ğ‘¡ . Este erro dita qual a performance final do
modelo criado, feitas todas as otimizaÃ§Ãµes planejadas pelo autor. Importante: O cÃ¡lculo
de erro de teste deve ser feito apenas uma vez, para ilustrar qual a performance do modelo
criado. Feito isso, otimizaÃ§Ãµes subsequentes devem ser performadas em outro conjunto de
teste, caso contrÃ¡rio a mÃ©trica de teste perde o seu valor.

11

CapÃ­tulo 3
Redes Neurais
O termo Redes Neurais tem suas origens em tentativas de encontrar representaÃ§Ãµes
matemÃ¡ticas de processamento de informaÃ§Ã£o em sistemas biolÃ³gicos (McCulloch and
Pitts, 1943), neste artigo os autores modelam um neurÃ´nio como um interruptor que
recebe impulsos de outros neurÃ´nios e, dependendo do peso total recebido Ã© ativado ou
permanece desativado. Este peso cuja entrada de outra cÃ©lula Ã© multiplicada corresponde
a uma sinapse (conexÃµes entre cÃ©lulas nervosas). Em 1960 os autores mostraram que tais
neurÃ´nios possuem propriedades similares ao cÃ©rebro: eles conseguiam identificar padrÃµes
complexos ainda que alguns neurÃ´nios fossem destruÃ­dos.
Mais tarde este mesmo conceito foi utilizado para leitura de texto por mÃ¡quinas, em um
trabalho que despertou o avanÃ§o da Ã¡rea de aprendizado de mÃ¡quina, resultado em diversas
pesquisas acadÃªmicas e aplicaÃ§Ãµes diferentes que aplicavam o mesmo conceito.

3.1

FunÃ§Ãµes Feed-Forward

Matematicamente, o modelo mais bÃ¡sico dessa arquitetura pode ser descrito como
uma sÃ©rie de transformaÃ§Ãµes funcionais. Primeiro construÃ­mos ğ‘€ combinaÃ§Ãµes lineares
do vetor de entrada x = {ğ‘¥1 , ..., ğ‘¥ğ· }:
ğ·

ğ‘ğ‘— = âˆ‘ ğ‘¤ğ‘—ğ‘–(1) ğ‘¥ğ‘– + ğ‘¤ğ‘—0(1)
ğ‘–=1

onde ğ‘— = 1, ..., ğ‘€ e (1) indica que os parÃ¢metros estÃ£o na primeira â€™camadaâ€™ da rede.
AlÃ©m disso, vamos nos referir aos parÃ¢metros ğ‘¤ğ‘—ğ‘– como pesos e os parÃ¢metros ğ‘¤ğ‘—0 como viÃ©s.
Os valores ğ‘ğ‘— sÃ£o denominados ativaÃ§Ãµes. Cada uma dessas ativaÃ§Ãµes sÃ£o transformadas
utilizando uma funÃ§Ã£o diferenciÃ¡vel e nÃ£o-linear â„(.) retornando:
ğ‘§ğ‘— = â„(ğ‘ğ‘— )
Os valores ğ‘§ğ‘— correspondem Ã s unidades ocultas de uma das camadas da rede, configurando
assim uma camada oculta. Estes valores entÃ£o sÃ£o combinados novamente para retornar

12
3 | REDES NEURAIS

as unidades de ativaÃ§Ã£o de saÃ­da:
ğ‘€
(2)
ğ‘ğ‘˜ = âˆ‘ ğ‘¤ğ‘—ğ‘˜(2) ğ‘§ğ‘— + ğ‘¤ğ‘˜0
ğ‘—=1

onde ğ‘˜ = 1, ..., ğ¾ e ğ¾ Ã© o tamanho de saÃ­da desejada. Finalmente, os valores ğ‘ğ‘˜ sÃ£o transformados em valores de saÃ­da ğ‘¦ğ‘˜ utilizando uma funÃ§Ã£o de ativaÃ§Ã£o apropriada para o
problema em questÃ£o.
Esta arquitetura Ã© denominada Totalmente Conectada.

3.2

Backpropagation

O termo Backpropagation se refere ao algoritmo utilizado no treinamento de redes
neurais (Hecht-Nielsen, 1992). O objetivo Ã© calcular eficientemente o gradiente de uma
funÃ§Ã£o de erro ğ¸(ğ‘¤) para posteriormente atualizar os parÃ¢metros da rede de forma a
minimizar o erro. A forma mais simples de realizar este procedimento foi introduzida por
Rumelhart et al. (1986) e envolve o gradiente descendente.
O gradiente âˆ‡ğ‘“ de uma funÃ§Ã£o diferenciÃ¡vel ğ‘“ define um vetor com o sentido de maior
crescimento da funÃ§Ã£o ğ‘“ . Definimos âˆ’âˆ‡ğ‘“ como o gradiente descendente da funÃ§Ã£o ğ‘“ , naturalmente o gradiente descendente define um vetor com o sentido de maior decrescimento
da funÃ§Ã£o ğ‘“ . Uma vez que o objetivo Ã© minimizar a funÃ§Ã£o de erro, tomamos o gradiente
descendente como sentido de atualizaÃ§Ã£o dos parÃ¢metros de peso w.
Como exemplo, podemos realizar a seguinte sequÃªncia de passos para otimizar uma
rede Feed-Fordward com funÃ§Ã£o de ativaÃ§Ã£o ğ‘¡ğ‘ğ‘›â„ e funÃ§Ã£o de erro ğ‘€ğ‘†ğ¸:
1. Primeiro realizamos um passo de avaliaÃ§Ã£o, calculando:
ğ·

ğ‘ğ‘— = âˆ‘ ğ‘¤ğ‘–ğ‘—(1) ğ‘¥ğ‘–
ğ‘–=0

ğ‘§ğ‘— = ğ‘¡ğ‘ğ‘›â„(ğ‘ğ‘— )
ğ‘€

ğ‘¦ğ‘˜ = âˆ‘ ğ‘¤ğ‘˜ğ‘—(2) ğ‘§ğ‘—
ğ‘—=0

2. Feito isso, calculamos o gradiente para cada unidade de saÃ­da, utilizando:
ğ›¿ğ‘˜ = ğ‘¦ğ‘˜ âˆ’ ğ‘¡ğ‘˜
3. Com esses valores calculamos o gradiente para cada unidade na camada oculta:
ğ›¿ğ‘— = (1 âˆ’ ğ‘§ğ‘—2 ) âˆ‘ ğ‘¤ğ‘˜ğ‘— ğ›¿ğ‘˜
4. Finalmente, calculamos as derivadas em relaÃ§Ã£o aos pesos da primeira e segunda

13
3.3 | FUNÃ‡Ã•ES DE ATIVAÃ‡ÃƒO

camada, dados por:

ğ›¿ğ¸ğ‘›
ğ›¿ğ‘¤ğ‘—ğ‘–(1)
ğ›¿ğ¸ğ‘›
ğ›¿ğ‘¤ğ‘˜ğ‘—(2)

= ğ›¿ğ‘— ğ‘¥ ğ‘–
= ğ›¿ğ‘˜ ğ‘§ ğ‘—

Onde:
â€¢ ğ‘¦ğ‘˜ Ã© o resultado da ativaÃ§Ã£o da unidade ğ‘˜.
â€¢ ğ‘¡ğ‘˜ Ã© o objetivo correspondente.
â€¢ ğ‘¡ğ‘– Ã© um exemplo do vetor de entrada.
Repetindo esses passos um determinado nÃºmero de vezes ğµ retorna uma rede localmente
otimizada para o conjunto de exemplos em questÃ£o.

3.3

FunÃ§Ãµes de AtivaÃ§Ã£o

FunÃ§Ãµes de ativaÃ§Ã£o sÃ£o equaÃ§Ãµes que determinam as ativaÃ§Ãµes de nÃ³s de uma rede
neural. AlÃ©m de tomar a decisÃ£o de como passar um estÃ­mulo para as camadas subsequentes essas funÃ§Ãµes tambÃ©m sÃ£o responsÃ¡veis por mapear as estÃ­mulos para intervalos
menores, como o intervalo [0..1] no caso de um classificador. Essas funÃ§Ãµes devem ser
computacionalmente rÃ¡pidas de calcular, assim como suas derivadas, uma vez que durante
o treinamento elas serÃ£o utilizadas milhares, ou atÃ© milhÃµes de vezes para otimizar os
parÃ¢metros de uma rede.
Estas funÃ§Ãµes podem ser lineares ou nÃ£o-lineares. O uso de funÃ§Ãµes nÃ£o-lineares Ã©
recomendado em casos onde o conjunto de treinamento apresenta padrÃµes complexos,
uma vez que com a combinaÃ§Ã£o dessas funÃ§Ãµes podemos aproximar funÃ§Ãµes lineares e
nÃ£o-lineares.
A seguir, daremos detalhes sobre as funÃ§Ãµes de ativaÃ§Ã£o utilizadas nesse projeto.

3.3.1

Softmax

A funÃ§Ã£o softmax unitÃ¡ria ğœ âˆ¶ â„ğ‘˜ â†’ â„ğ‘˜ Ã© definida como:
ğœ (ğ‘§)ğ‘– =

ğ‘’ ğ‘§ğ‘–
âˆ‘ğ¾ğ‘—âˆ’1 ğ‘’ ğ‘§ğ‘—

onde ğ‘– = 1, ..., ğ¾
Tipicamente esta funÃ§Ã£o Ã© utilizada somente para camadas de saÃ­da, uma vez que ela Ã©
capaz de lidar com mÃºltiplas classes e mapear uma saÃ­da normalizada entre 1 e 0, consistindo
da probabilidade de que um exemplo da entrada seja de uma classe especÃ­fica.

14
3 | REDES NEURAIS

3.3.2

Rectified Linear Unit (ReLU)

A funcÃ£o relu Ã© definida como a parte positiva de seu argumento, isto Ã©:
ğ‘Ÿğ‘’ğ‘™ğ‘¢(ğ‘¥) = ğ‘šğ‘ğ‘¥(0, ğ‘¥)
Estas funÃ§Ãµes geralmente sÃ£o utilizadas em camadas de entrada e ocultas e sÃ£o populares
devido Ã  eficiÃªncia de cÃ¡lculo e a nÃ£o-linearidade. Juntos, esses dois fatores fazem com que
redes que utilizam essa funÃ§Ã£o tenham rÃ¡pida convergÃªncia e alta capacidade de capturar
padrÃµes.

3.4

Redes Neurais Convolucionais

Com respeito Ã s caracterÃ­sticas extraÃ­das da imagem e que sÃ£o utilizadas como entrada
pelos algoritmos de aprendizado de mÃ¡quina, muitas sÃ£o computadas aplicando-se operaÃ§Ãµes de convoluÃ§Ã£o. ConvoluÃ§Ãµes com determinadas mÃ¡scaras sÃ£o Ãºteis, por exemplo,
para realÃ§ar contornos.
A ideia de Redes Neurais Convolucionais surge com LeCun et al., 1999 em uma tentativa
de criar um modelo que Ã© inerente a certas transformaÃ§Ãµes na entrada. No caso de imagens
por exemplo, podemos ter o interesse de ter uma mesma prediÃ§Ã£o de classe para exemplos
rotacionados, ou com leves distorÃ§Ãµes. Redes neurais tradicionais ignoram o fato de que
pixels em uma vizinhanÃ§a pequena compartilham mais informaÃ§Ãµes entre si do que pixels
distantes um do outro. AlÃ©m disso, caracterÃ­sticas que sÃ£o Ãºteis em uma regiÃ£o podem
tambÃ©m ser Ãºteis em outras regiÃµes.
Essas noÃ§Ãµes sÃ£o incorporadas por CNNs atravÃ©s de 3 mecanismos: campos receptivos
locais, compartilhamento de pesos e sub-amostragem. Em uma camada convolucional as
unidades sÃ£o organizadas em planos chamados de mapa de features. Cada unidade em um
mapa recebe informaÃ§Ãµes apenas de uma regiÃ£o pequena da imagem.
Se interpretarmos as unidades como detectores de caracterÃ­stiscas, entÃ£o todas as
unidades em um mesmo mapa detectam um mesmo padrÃ£o, mas em locais diferentes
da imagem de entrada. Devido ao compartilhamento de pesos, a avaliaÃ§Ã£o das ativaÃ§Ãµes
dessas unidades Ã© equivalente a uma convoluÃ§Ã£o da intensidade de cor do pixel da imagem
com um kernel contendo os parÃ¢metros de pesos.

Figura 3.1: Um exemplo de convoluÃ§Ã£o.

15
3.5 | OPERAÃ‡Ã•ES DE CONVOLUÃ‡ÃƒO

3.5

OperaÃ§Ãµes de ConvoluÃ§Ã£o

ConvoluÃ§Ã£o Ã© uma operaÃ§Ã£o que expressa a quantidade de sobreposiÃ§Ã£o entre duas
funÃ§Ãµes ğ‘“ e ğ‘”. Em termos matemÃ¡ticos, definimos a convoluÃ§Ã£o ğ‘“ âˆ— ğ‘” como:
ğ‘¡

[ğ‘“ âˆ— ğ‘”](ğ‘¡) = âˆ«

ğ‘“ (ğœ )ğ‘”(ğ‘¡ âˆ’ ğœ )ğ›¿ğœ

0

Em processamento de imagens no entanto, operaÃ§Ãµes de convoluÃ§Ã£o sÃ£o utilizadas
para diferentes tarefas como borrar, realÃ§ar detalhes e detectar contornos de objetos. Para
isso, sÃ£o definidas matrizes denominadas como kernel e Ã© realizada a convoluÃ§Ã£o pixel a
pixel, da imagem com o kernel escolhido seguindo a equaÃ§Ã£o:
ğ‘

ğ‘

ğ‘”(ğ‘¥, ğ‘¦) = ğ‘¤ âˆ— ğ‘“ (ğ‘¥, ğ‘¦) = âˆ‘ âˆ‘ ğ‘¤(ğ‘ , ğ‘¡)ğ‘“ (ğ‘¥ âˆ’ ğ‘ , ğ‘¦ âˆ’ ğ‘¡)
ğ‘ =âˆ’ğ‘ ğ‘¡=âˆ’ğ‘

onde ğ‘¤ Ã© o kernel, ğ‘”(ğ‘¥, ğ‘¦) Ã© a imagem resultante e ğ‘“ (ğ‘¥, ğ‘¦) Ã© a imagem original.
Note que o resultado da convoluÃ§Ã£o depende unicamente do kernel utilizado. Para
o prÃ©-processamento de imagens existem uma sÃ©rie de kernels que podem ser obtidos
atravÃ©s de equaÃ§Ãµes matemÃ¡ticas.

3.6

Redes Neurais Totalmente Convolucionais

Uma das formas de segmentar imagens de forma rÃ¡pida e computacionalmente eficiente Ã© atravÃ©s de redes Redes Neurais Totalmente Convolucionais ou FCNs. Estas redes
consistem em suscetivas transformaÃ§Ãµes convolucionais aplicadas Ã  janelas de pixel ğ‘Š que
tÃªm como objetivo, ou conjunto verdade, uma janela de pixels denominada patch.
Como Shelhamer et al., 2016 descreve em seu artigo â€œFully Convolutional Networks for
Semantic Segmentationâ€, cada camada em uma FCN consiste de um vetor tridimensional
de tamanho â„ Ã— ğ‘¤ Ã— ğ‘‘ onde â„ e ğ‘¤ sÃ£o dimensÃµes espaciais e ğ‘‘ Ã© a dimensÃ£o do canal
ou feature. Diferentemente de CNNs para segmentaÃ§Ã£o padrÃ£o, FCNs nÃ£o utilizam-se de
tÃ©cnicas de upsampling, nem de arquiteturas no estilo bottleneck. Aqui, tomamos vantagem
do fato de que sub-amostras (janelas) da imagem podem ser utilziadas como conjunto
de treino para a rede neural, e assim podemos fazer a segmentaÃ§Ã£o da imagem inteira
atravÃ©s da segmentaÃ§Ã£o dessas janelas retiradas da imagem, o que resulta em tempos de
processamento melhores e menor quantidade de memÃ³ria alocada.

17

CapÃ­tulo 4
Metodologia
Como descrito no tÃ­tulo, podemos dividir este trabalho duas grandes etapas: SegmentaÃ§Ã£o e SÃ­ntese. Para realizar a segmentaÃ§Ã£o de imagens, treinamos diferentes classificadores
de mesma arquitetura mas com conjuntos de treino (campos receptivos) diferentes. Estes
campos receptivos sÃ£o janelas de pixels com centro deslocado em alguns pixels em relaÃ§Ã£o
ao seu objetivo.
Ao realizar este deslocamento, conseguimos capturar um campo receptivo maior, sem
perdas de generalidade. Contudo, se faz necessÃ¡rio um mecanismo de sÃ­ntese de saÃ­das, uma
vez que cada classificador darÃ¡ um resultado diferente para um mesmo exemplo de entrada.
Neste sentido, exploramos diferentes maneiras de combinar estes classificadores.

Figura 4.1: Esquema final desejado.

No sentido de implementaÃ§Ã£o utilizamos as definiÃ§Ãµes disponÃ­veis pela biblioteca Keras,
utilizando Tensorflow como plataforma backend.

4.1

SegmentaÃ§Ã£o

Neste trabalho utilizamos redes convolucionais com arquitetura FCN para realizar o
trabalho de segmentaÃ§Ã£o, utilizando Softmax como camada de ativaÃ§Ã£o. Cada rede toma
como entrada uma janela ğ¾ de pixels de tamanho 27 Ã— 27 e um canal de cor, e mapeia um
conjunto de pixels ğ‘€ de tamanho 5 Ã— 5, restrito ao conjunto binÃ¡rio {0, 1}.
Considerando ğ¶ como o nÃºmero total de classificadores e ğ‘ğ‘–,ğ‘— como a classe real de um
pixel ğ‘ na posiÃ§Ã£o (ğ‘–, ğ‘—), a responsabilidade de cada um destes classificadores Ã© atribuir

18
4 | METODOLOGIA

uma classe ğ‘Ì‚ğ‘–,ğ‘— para o pixel ğ‘ğ‘–,ğ‘— . Devido ao fato de que utilizamos Softmax como camada de
ativaÃ§Ã£o, ğ‘Ì‚ğ‘–,ğ‘— pode ser obtido atravÃ©s da seguinte operaÃ§Ã£o:
{
0 se ğ‘ ğ‘–,ğ‘— < 0.5
ğ‘Ì‚ğ‘–,ğ‘— =
(4.1)
1 caso contrÃ¡rio
Onde ğ‘ ğ‘–,ğ‘— denota a saÃ­da da rede, contendo a probabilidade de um pixel pertencer Ã 
classe 1.
Com isso, podemos resumir esta etapa como uma funÃ§Ã£o:
5Ã—5Ã—ğ¶
ğ‘†ğ‘” âˆ¶ â„¤27Ã—27
0â‰¤ğ‘¥â‰¤255 â†’ â„0â‰¤ğ‘¥â‰¤1

Esta funÃ§Ã£o Ã© aplicada para todas as janelas amostradas a partir da imagem inicial,
gerando assim um conjunto de C matrizes de dimensÃ£o igual Ã  imagem inicial, contendo
as probabilidades de cada pixel pertencer Ã  classe 1.

4.1.1

Arquitetura de SegmentaÃ§Ã£o

No sentido de arquitetura de redes neurais, utilizamos 5 camadas internas de convoluÃ§Ã£o, com kernel de tamanho 3 Ã— 3 e com ativaÃ§Ãµes do tipo relu. Utilizamos entropia cruzada
como medida de otimizaÃ§Ã£o e learning rate adaptativo (Zeiler, 2012). A saÃ­da da rede de
classificaÃ§Ã£o utilizada foi Softmax. Esta funÃ§Ã£o nos permite extrair o grau de confianÃ§a do
modelo em uma prediÃ§Ã£o, que mais tarde serÃ¡ utilizado para realizar o voto majoritÃ¡rio.
Mais detalhes de arquitetura podem ser consultados na figura abaixo.

Figura 4.2: Rede convolucional de segmentaÃ§Ã£o.

4.2

SÃ­ntese

Podemos definir como funÃ§Ã£o de sÃ­ntese uma funÃ§Ã£o ğ‘†ğ‘– âˆ¶ â„ğ¶ â†’ â„¤0,1 . Onde ğ¶ Ã© o
nÃºmero de classificadores. Ao aplicar essa funÃ§Ã£o pixel a pixel em cada um dos vetores

19
4.2 | SÃNTESE

gerados pela sobreposiÃ§Ã£o das imagens das janelas de saÃ­da de redes, podemos criar a
seguinte abstraÃ§Ã£o de classificaÃ§Ã£o:
5Ã—5
ğ‘†ğ‘” â—¦ğ‘†ğ‘– âˆ¶ â„¤27Ã—27
0â‰¤ğ‘¥â‰¤255 â†’ â„¤0,1

Com isso, basta aplicar essa funÃ§Ã£o para todas as janelas amostradas a partir da imagem
inicial para ter o resultado final de uma imagem segmentada.
Existem diferentes formas de criar tais funÃ§Ãµes de sÃ­ntese ğ‘†ğ‘– . Neste trabalho exploramos
os seguintes: voto majoritÃ¡rio, voto ponderado e rede neural de sÃ­ntese.

4.2.1

Voto MajoritÃ¡rio

Como o nome sugere, o voto majoritÃ¡rio consiste em atribuir como classe final ao pixel
a classe mais votada entre os classificadores para aquele pixel, ou seja:
ğ‘

ğ‘€ = âˆ‘ ğœÌ‚ ğ¢,ğ£ğ§
ğ‘›=0

{
ğ‘†ğ‘–ğ‘š (ğœğ¢,ğ£ )

0 se ğ‘€/ğ‘ < 0.5
1 caso contrÃ¡rio

(4.2)

Onde ğœÌ‚ ğ¢,ğ£ denota o vetor de classes atribuÃ­das para o pixel ğ‘ğ‘–,ğ‘— pelos classificadores atravÃ©s
do procedimento descrito na equaÃ§Ã£o 4.1 e N denota o nÃºmero de classificadores.
Note que em casos de empate no nÃºmero de votos de classes este classificador arredonda
o valor para a classe 0, e este procedimento sÃ³ faz sentido em casos de classificadores binÃ¡rios. Para evitar empates neste trabalho empregamos um nÃºmero Ã­mpar de classificadores
em uma tarefa com duas classes.

4.2.2

Voto Ponderado

O voto ponderado toma vantagem do fato de que os classificadores possuem um grau
de confianÃ§a ğ¬ğ¢,ğ£ na prediÃ§Ã£o. Com essa informaÃ§Ã£o podemos retornar como resposta a
classe com maior grau de confianÃ§a, isto Ã©:
ğ‘

ğ‘ƒ = âˆ‘ ğœÌ‚ ğ¢,ğ£ğ§ âˆ— ğ¬ğ¢,ğ£ğ§
ğ‘›=0

{
0
ğ‘†ğ‘–ğ‘ (ğœğ¢,ğ£ )
1

se ğ‘ƒ < 0.5
caso contrÃ¡rio

Novamente, este procedimento sÃ³ faz sentido em casos de classificadores binÃ¡rios

(4.3)

20
4 | METODOLOGIA

4.2.3

Rede Neural de SÃ­ntese

Para utilizar uma rede neural de sÃ­ntese definimos uma rede neural no estilo Fully
Connected com nÃºmero de canais de entrada igual ao nÃºmero de classificadores e nÃºmero
de canais de saÃ­da igual ao nÃºmero de classes desejadas. Naturalmente, existem diferentes
arquiteturas que podem ser utilizadas para realizar este procedimento, e devem ser levados
em conta o nÃºmero de classes, o nÃºmero de exemplos e a quantidade de classificadores.
Neste trabalho, utilizamos apenas uma camada interna, com 10 tensores.

Figura 4.3: Rede neural de SÃ­ntese.

Ao utilizar uma rede neural de sÃ­ntese Ã© possÃ­vel identificar padrÃµes de sÃ­ntese de
redes, isto Ã©, se torna possÃ­vel que o voto ponderado em um pixel nÃ£o seja o resultado
final da classe naquele pixel, devido ao fato que determinadas combinaÃ§Ãµes de saÃ­da de
classificadores podem ser mais relevantes que outras.
Sobre as ativaÃ§Ãµes, utilizamos relu e entropia cruzada nesta rede. A funÃ§Ã£o de saÃ­da
utilizada foi Softmax, porÃ©m podemos utilizar funÃ§Ã£o de saÃ­da sem perda de generalidade,
uma vez que estamos interessados somente em qual classe serÃ¡ atribuida para o conjunto
de pixels proveniente da fase de classificaÃ§Ã£o. Desta vez utilizamos adam (Kingma e Ba,
2014) como otimizador de learning rate.

21

CapÃ­tulo 5
Resultados
Aplicamos a arquitetura definida na metodologia em exemplos de segmentaÃ§Ã£o de
quadrinhos. Cada um dos exemplos de entrada constitui uma pÃ¡gina de tamanho 1170 Ã— 827
escaneada de uma revista de quadrinhos mangÃ¡. O conjunto verdade Ã© constituÃ­do por
uma imagem de tamanho de tamanho 1170 Ã— 827 com pixels de valor binÃ¡rio: aqueles que
fazem parte de uma representaÃ§Ã£o de texto recebem label 1 e o restante recebe label 0.
Para simplificar o processo, estas pÃ¡ginas foram transformadas em tons de cinza antes de
serem processadas pelo modelo de segmentaÃ§Ã£o.
Ao total, utilizamos 5 classificadores para a segmentaÃ§Ã£o, sendo que cada um deles
teve um deslocamento em relaÃ§Ã£o ao patch objetivo:
1. Sup Left: O objetivo fica no canto superior esquerdo da janela.
2. Sup Right: O objetivo fica no canto superior direito da janela.
3. Bot Left: O objetivo fica no canto inferior esquerdo da janela.
4. Bot Right: O objetivo fica no canto inferior direito da janela.
5. Center: O objetivo fica no centro da janela.
Quando combinadas, as janelas capturam um campo receptivo de tamanho total 41 Ã— 41
para um mesmo objetivo de tamanho 5 Ã— 5.
O conjunto de treino dos modelos de segmentaÃ§Ã£o consistiu de um conjunto de 15
imagens do dataset, separadas em janelas de tamanho 27 Ã— 27. Validamos este modelo
utilizando amostras de 5 imagens. Para treinar o modelo de sÃ­ntese, avaliamos 15 imagens
com os classificadores, concatenamos e utilizamos como conjunto de treino os vetores
de tamanho 1 Ã— 5 contendo cada uma das prediÃ§Ãµes dos classificadores para um mesmo
pixel.
A acurÃ¡cia mÃ©dia final no conjunto de validaÃ§Ã£o dos classificadores foi 0.994 enquanto
que da rede neural de sÃ­ntese foi 0.991, isto Ã©, em aproximadamente 99% dos casos o pixel
dado pela rede de classificaÃ§Ã£o foi correto e o mesmo vale para a rede de sÃ­ntese.
Treinadas ambas as redes, utilizamos um conjunto de 30 imagens para avaliar cada um
dos classificadores. O resultado foi o seguinte:

22
5 | RESULTADOS

Model
Sup Left
Sup Right
Bot Left
Bot Right
Center
Weig Avg
Neur Net

Model List
LogLoss
234.1425
876.3428
288.8897
107.5662
101.3952
290.615
81.175

Acc
0.9667
0.930
0.9668
0.9834
0.9719
0.987
0.985

Tabela 5.1: Modelos e mÃ©tricas correspondentes no conjunto de teste.

Em termos de Erro LogarÃ­tmico podemos perceber pela tabela que, em mÃ©dia, a rede
neural de combinaÃ§Ã£o atingiu um erro equivalente a pouco menos de um terÃ§o da mÃ©dia
das redes neurais sozinhas. Este resultado contudo, nÃ£o se reflete totalmente na acurÃ¡cia
do modelo. Em termos de acurÃ¡cia na prediÃ§Ã£o de pixels, o modelo de combinaÃ§Ã£o de
redes ficou em segundo lugar, logo apÃ³s o de voto ponderado. Esta performance contudo,
supera a mÃ©dia dos outros sistemas de classificaÃ§Ã£o em uma mÃ©dia de 2.5% de acurÃ¡cia. Esta
alteraÃ§Ã£o se reflete na segmentaÃ§Ã£o. Como podemos ver na figura, o modelo de segmentaÃ§Ã£o
conseguiu abstrair melhor as figuras quando comparado com um dos modelos.
AlÃ©m disso, tambÃ©m foi possÃ­vel ver que a performance cai quando alteramos o peso
entre as classes. Isto provavelmente Ã© resultado do fato de que, ao aumentar o peso da
classe 1 (Branca) melhoramos a visibilidade do texto na imagem, mas tambÃ©m aumentamos
o ruÃ­do no resultado. Neste trabalho nÃ£o foi possÃ­vel explorar maneiras de aumentar a
qualidade do texto sem aumentar o ruÃ­do.
A Figura 4 ilustra a distribuiÃ§Ã£o de acurÃ¡cia para o conjunto de testes. AtravÃ©s dela,
podemos concluir que o modelo com combinaÃ§Ã£o de resultados obteve resultados consistentemente melhores que os modelos individuais.

23

CapÃ­tulo 6
ConclusÃ£o
A Ã¡rea de aprendizado de mÃ¡quina ainda possui muitos temas nÃ£o explorados, que
podem render Ã³timos resultados para o campo acadÃªmico. Neste trabalho conseguimos
explorar um destes temas, utilizando tÃ©cnicas comuns como CNNs e Redes Neurais para
chegar a um resultado melhor do que algumas das tÃ©cnicas empregadas hoje no espaÃ§o
de segmentaÃ§Ã£o de imagens, sem a necessidade de empregar arquiteturas de aprendizado
profundo. O tempo de execuÃ§Ã£o e eficiÃªncia de algoritmos de aprendizado de mÃ¡quina vem
se tornando componente essencial para que se torne possÃ­vel a aplicaÃ§Ã£o destes modelos
em tarefas do dia a dia.
Particularmente, este Ã© um Ã³timo exemplo de como podemos compor diferentes algoritmos de aprendizado de mÃ¡quina em partes de uma mesma tarefa. Este desacoplamento
nos permitiu inserir novos elementos de complexidade que geraram melhores resultados. Ã‰
fato particular que a melhora do erro logarÃ­tmico mÃ©dio no conjunto de testes nÃ£o resultou
necessariamente em uma melhora da acurÃ¡cia mÃ©dia. Contudo, Ã© natural imaginar que a
sÃ­ntese por meio de redes neurais teria um melhor erro, visto que durante o treino essa Ã©
a mÃ©trica utilizada para otimizar o modelo. O uso de mÃ©tricas diferentes para treino de
ambas as arquiteturas de rede nÃ£o foi explorado neste trabalho, mas Ã© algo que pode gerar
performances ainda melhores daquelas que foram reportadas aqui.
Dentro deste mesmo trabalho ainda restam alguns tÃ³picos interessantes e que podem
gerar novas pesquisas. Aqui foi possÃ­vel perceber que existe uma relaÃ§Ã£o direta entre
o peso atribuÃ­do Ã s classes e a qualidade da sÃ­ntese. AlÃ©m disso, tambÃ©m percebemos
que classificadores com campos receptivos levemente deslocados podem ter performance
melhor em alguns casos, e que a combinaÃ§Ã£o de tais classificadores Ã© tarefa de certa
complexidade e que requer estudos para melhor entender quais as melhores tÃ©cnicas e
maneiras de realizar a tarefa. Seguindo com este estudo ainda se faz necessÃ¡rio juntar as
duas redes neurais em um mesmo pipeline, de forma a otimizar o processo de segmentaÃ§Ã£o
fim-a-fim.
Finalmente, agradeÃ§o Ã  todos aqueles que contribuÃ­ram para que este trabalho fosse um
sucesso. Particularmente aos orientadores deste trabalho que demonstraram dedicaÃ§Ã£o e
conhecimento no assunto sempre que requisitados, Ã  Amazon por prover os recursos computacionais necessÃ¡rios e ao Instituto de MatemÃ¡tica e EstatÃ­stica da USP por disponibilizar

24
6 | CONCLUSÃƒO

os espaÃ§os de estudo e trabalho utilizados durante o ano.

25
6 | CONCLUSÃƒO

Figura 1: Resultado final do trabalho: Entrada Ã  esquerda, Ground Truth ao centro e saÃ­da da
combinaÃ§Ã£o de redes Ã  direita.

Figura 2: Entrada e saÃ­da dos modelos: Ã€ esquerda temos o quadrinho e a direita o Ground Truth.
A segunda imagem representa a saÃ­da esquema de Redes Neurais e a terceira imagem a saÃ­da obtida
atravÃ©s do mÃ©todo de voto ponderado

26
6 | CONCLUSÃƒO

Figura 3: Resultados das redes individuais aplicados a um exemplo de imagem.

Figura 4: DistribuiÃ§Ã£o de acurÃ¡cias de cada um dos modelos. Da esquerda para a direita temos:
Rede Neural, Voto Ponderado e os classificadores individuais na ordem descrita na seÃ§Ã£o 5.

27

ReferÃªncias
[Hecht-Nielsen 1992] Robert Hecht-Nielsen. â€œTheory of the backpropagation neural
networkâ€. Em: Neural networks for perception. Elsevier, 1992, pgs. 65â€“93 (citado na
pg. 12).
[Hirata 2008] Nina ST Hirata. â€œMultilevel training of binary morphological operatorsâ€.
Em: IEEE Transactions on pattern analysis and machine intelligence 31.4 (2008),
pgs. 707â€“720 (citado na pg. 2).
[Kingma e Ba 2014] Diederik P. Kingma e Jimmy Ba. Adam: A Method for Stochastic
Optimization. 2014. arXiv: 1412.6980 [cs.LG] (citado na pg. 20).
[LeCun et al. 1999] Yann LeCun, Patrick Haffner, LÃ©on Bottou e Yoshua Bengio.
â€œObject recognition with gradient-based learningâ€. Em: Shape, contour and grouping
in computer vision. Springer, 1999, pgs. 319â€“345 (citado na pg. 14).
[Lu e Shridhar 1996] Yi Lu e Malayappan Shridhar. â€œCharacter segmentation in
handwritten wordsâ€”an overviewâ€. Em: Pattern recognition 29.1 (1996), pgs. 77â€“96
(citado na pg. 1).
[Mustaqeem et al. 2012] Anam Mustaqeem, Ali Javed e Tehseen Fatima. â€œAn efficient
brain tumor detection algorithm using watershed & thresholding based segmentationâ€. Em: International Journal of Image, Graphics and Signal Processing 4.10
(2012), pg. 34 (citado na pg. 1).
[Shelhamer et al. 2016] Evan Shelhamer, Jonathan Long e Trevor Darrell. â€œFully
convolutional networks for semantic segmentationâ€. Em: PAMI (2016) (citado na
pg. 15).
[Solomonoff 1957] Ray Solomonoff. â€œAn inductive inference machineâ€. Em: IRE
Convention Record, Section on Information Theory, Part 2 (1957), pgs. 56â€“62 (citado
na pg. 6).
[Zeiler 2012] Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. 2012.
arXiv: 1212.5701 [cs.LG] (citado na pg. 18).

